{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas spacy -q"
      ],
      "metadata": {
        "id": "YZyxFyVWjUer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 -q"
      ],
      "metadata": {
        "id": "K_Ks_37PjgF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXnv0lhiji1o",
        "outputId": "ed19d61f-0186-42a0-9ae2-0407fdb55fce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1M0GW8TjIVJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "import json\n",
        "import pickle\n",
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    \"\"\"Custom Dataset for batch processing\"\"\"\n",
        "    def __init__(self, texts, tokenizer, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        # Truncate text if too long\n",
        "        if len(text) > 500:\n",
        "            text = text[:500]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'idx': idx\n",
        "        }\n",
        "\n",
        "class PoliticalBiasAnalyzer:\n",
        "    \"\"\"\n",
        "    Optimized Political Bias Analysis System using Batch Processing\n",
        "    Analyzes bias towards BJP and Congress using NER, sentiment, and framing analysis\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, batch_size=32, device=None):\n",
        "        print(\"Initializing Political Bias Analyzer...\")\n",
        "\n",
        "        # Set device\n",
        "        if device is None:\n",
        "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        else:\n",
        "            self.device = torch.device(device)\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Load spaCy model\n",
        "        try:\n",
        "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "        except:\n",
        "            print(\"Downloading spaCy model...\")\n",
        "            import os\n",
        "            os.system(\"python -m spacy download en_core_web_sm\")\n",
        "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "        # Load sentiment analysis model and tokenizer\n",
        "        print(\"Loading sentiment analysis model...\")\n",
        "        model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "        # Political entity mappings\n",
        "        self.BJP_ENTITIES = {\n",
        "            \"bjp\", \"bharatiya janata party\", \"narendra modi\", \"modi\", \"pm modi\",\n",
        "            \"amit shah\", \"shah\", \"rajnath singh\", \"nirmala sitharaman\",\n",
        "            \"yogi adityanath\", \"adityanath\", \"jp nadda\", \"nda\", \"national democratic alliance\"\n",
        "        }\n",
        "\n",
        "        self.CONGRESS_ENTITIES = {\n",
        "            \"congress\", \"indian national congress\", \"inc\", \"rahul gandhi\", \"rahul\",\n",
        "            \"sonia gandhi\", \"sonia\", \"priyanka gandhi\", \"priyanka\", \"mallikarjun kharge\",\n",
        "            \"kharge\", \"upa\", \"united progressive alliance\"\n",
        "        }\n",
        "\n",
        "        # Opinion and hedge markers\n",
        "        self.OPINION_MARKERS = {\n",
        "            \"think\", \"believe\", \"assume\", \"claim\", \"allegedly\", \"reportedly\",\n",
        "            \"critics say\", \"observers note\", \"analysts suggest\", \"appears to be\",\n",
        "            \"seems to\", \"purportedly\", \"supposedly\", \"opinion\", \"viewed as\"\n",
        "        }\n",
        "\n",
        "        self.HEDGE_WORDS = {\n",
        "            \"perhaps\", \"possibly\", \"might\", \"could\", \"may\", \"likely\",\n",
        "            \"probably\", \"seemingly\", \"apparently\", \"suggests\", \"indicates\"\n",
        "        }\n",
        "\n",
        "        # Framing words\n",
        "        self.NEGATIVE_FRAMES = {\n",
        "            \"mob\", \"riot\", \"attack\", \"accuse\", \"blame\", \"controversy\",\n",
        "            \"scandal\", \"corruption\", \"failure\", \"crisis\", \"chaos\", \"turmoil\",\n",
        "            \"strongman\", \"authoritarian\", \"dictator\", \"suppress\", \"crackdown\"\n",
        "        }\n",
        "\n",
        "        self.POSITIVE_FRAMES = {\n",
        "            \"leader\", \"initiative\", \"reform\", \"development\", \"progress\",\n",
        "            \"achievement\", \"success\", \"growth\", \"innovation\", \"stability\"\n",
        "        }\n",
        "\n",
        "        # Results storage\n",
        "        self.results = {\n",
        "            'bjp': defaultdict(list),\n",
        "            'congress': defaultdict(list)\n",
        "        }\n",
        "\n",
        "        self.articles_analyzed = 0\n",
        "\n",
        "    def load_data(self, filepath, text_column='text', title_column=None):\n",
        "        \"\"\"Load dataset from CSV, JSON, or Excel files\"\"\"\n",
        "        print(f\"Loading data from {filepath}...\")\n",
        "\n",
        "        if filepath.endswith('.csv'):\n",
        "            self.df = pd.read_csv(filepath)\n",
        "        elif filepath.endswith('.json'):\n",
        "            self.df = pd.read_json(filepath)\n",
        "        elif filepath.endswith('.xlsx'):\n",
        "            self.df = pd.read_excel(filepath)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file format. Use CSV, JSON, or XLSX\")\n",
        "\n",
        "        self.text_column = text_column\n",
        "        self.title_column = title_column\n",
        "\n",
        "        print(f\"Loaded {len(self.df)} articles\")\n",
        "        return self.df\n",
        "\n",
        "    def identify_party(self, text):\n",
        "        \"\"\"Identify which party/parties are mentioned in text\"\"\"\n",
        "        text_lower = text.lower()\n",
        "        parties = []\n",
        "\n",
        "        for entity in self.BJP_ENTITIES:\n",
        "            if entity in text_lower:\n",
        "                parties.append('bjp')\n",
        "                break\n",
        "\n",
        "        for entity in self.CONGRESS_ENTITIES:\n",
        "            if entity in text_lower:\n",
        "                parties.append('congress')\n",
        "                break\n",
        "\n",
        "        return parties\n",
        "\n",
        "    def extract_context_window(self, text, entity_pos, window_size=2):\n",
        "        \"\"\"Extract sentences around entity mention\"\"\"\n",
        "        doc = self.nlp(text)\n",
        "        sentences = list(doc.sents)\n",
        "\n",
        "        # Find sentence containing entity\n",
        "        target_sent_idx = None\n",
        "        for idx, sent in enumerate(sentences):\n",
        "            if entity_pos >= sent.start_char and entity_pos <= sent.end_char:\n",
        "                target_sent_idx = idx\n",
        "                break\n",
        "\n",
        "        if target_sent_idx is None:\n",
        "            return text[:500]\n",
        "\n",
        "        # Extract window\n",
        "        start_idx = max(0, target_sent_idx - window_size)\n",
        "        end_idx = min(len(sentences), target_sent_idx + window_size + 1)\n",
        "\n",
        "        context = \" \".join([sent.text for sent in sentences[start_idx:end_idx]])\n",
        "        return context\n",
        "\n",
        "    def batch_sentiment_analysis(self, texts):\n",
        "        \"\"\"\n",
        "        Perform sentiment analysis on a batch of texts\n",
        "        Returns: list of (sentiment_label, score) tuples\n",
        "        \"\"\"\n",
        "        if not texts:\n",
        "            return []\n",
        "\n",
        "        # Create dataset and dataloader\n",
        "        dataset = TextDataset(texts, self.tokenizer)\n",
        "        dataloader = DataLoader(\n",
        "            dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=0  # Set to 0 to avoid multiprocessing issues\n",
        "        )\n",
        "\n",
        "        results = [None] * len(texts)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in dataloader:\n",
        "                input_ids = batch['input_ids'].to(self.device)\n",
        "                attention_mask = batch['attention_mask'].to(self.device)\n",
        "                indices = batch['idx'].numpy()\n",
        "\n",
        "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "\n",
        "                # Get predictions\n",
        "                scores, labels = torch.max(predictions, dim=1)\n",
        "\n",
        "                for idx, label, score in zip(indices, labels.cpu().numpy(), scores.cpu().numpy()):\n",
        "                    # Map labels (0: negative, 1: neutral, 2: positive)\n",
        "                    label_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
        "                    sentiment = label_map.get(label, 'neutral')\n",
        "                    results[idx] = (sentiment, float(score))\n",
        "\n",
        "        return results\n",
        "\n",
        "    def detect_opinion_markers(self, text):\n",
        "        \"\"\"Detect opinion and hedge words in text\"\"\"\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        opinion_count = sum(1 for marker in self.OPINION_MARKERS if marker in text_lower)\n",
        "        hedge_count = sum(1 for hedge in self.HEDGE_WORDS if hedge in text_lower)\n",
        "\n",
        "        has_opinion = opinion_count > 0 or hedge_count > 0\n",
        "\n",
        "        return {\n",
        "            'has_opinion': has_opinion,\n",
        "            'opinion_count': opinion_count,\n",
        "            'hedge_count': hedge_count\n",
        "        }\n",
        "\n",
        "    def analyze_framing(self, text):\n",
        "        \"\"\"Analyze framing bias using loaded language\"\"\"\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        negative_count = sum(1 for word in self.NEGATIVE_FRAMES if word in text_lower)\n",
        "        positive_count = sum(1 for word in self.POSITIVE_FRAMES if word in text_lower)\n",
        "\n",
        "        return {\n",
        "            'negative_frames': negative_count,\n",
        "            'positive_frames': positive_count,\n",
        "            'framing_score': positive_count - negative_count\n",
        "        }\n",
        "\n",
        "    def preprocess_articles(self):\n",
        "        \"\"\"\n",
        "        Preprocess all articles to extract contexts and party mentions\n",
        "        Returns: dictionary with contexts organized by party\n",
        "        \"\"\"\n",
        "        print(\"\\nPreprocessing articles to extract party contexts...\")\n",
        "\n",
        "        contexts_data = []\n",
        "\n",
        "        for idx, row in tqdm(self.df.iterrows(), total=len(self.df)):\n",
        "            text = row[self.text_column]\n",
        "\n",
        "            if pd.isna(text) or len(str(text).strip()) < 50:\n",
        "                continue\n",
        "\n",
        "            text = str(text)\n",
        "            parties_mentioned = self.identify_party(text)\n",
        "\n",
        "            if not parties_mentioned:\n",
        "                continue\n",
        "\n",
        "            for party in parties_mentioned:\n",
        "                entity_keywords = self.BJP_ENTITIES if party == 'bjp' else self.CONGRESS_ENTITIES\n",
        "\n",
        "                # Find first mention\n",
        "                text_lower = text.lower()\n",
        "                first_pos = len(text)\n",
        "                for keyword in entity_keywords:\n",
        "                    pos = text_lower.find(keyword)\n",
        "                    if pos != -1 and pos < first_pos:\n",
        "                        first_pos = pos\n",
        "\n",
        "                if first_pos == len(text):\n",
        "                    context = text[:500]\n",
        "                else:\n",
        "                    context = self.extract_context_window(text, first_pos)\n",
        "\n",
        "                contexts_data.append({\n",
        "                    'article_id': idx,\n",
        "                    'party': party,\n",
        "                    'context': context,\n",
        "                    'full_text': text\n",
        "                })\n",
        "\n",
        "        return contexts_data\n",
        "\n",
        "    def analyze_dataset(self):\n",
        "        \"\"\"Analyze entire dataset using batch processing\"\"\"\n",
        "        print(f\"\\nAnalyzing {len(self.df)} articles...\")\n",
        "\n",
        "        # Preprocess to get all contexts\n",
        "        contexts_data = self.preprocess_articles()\n",
        "\n",
        "        if not contexts_data:\n",
        "            print(\"No articles with party mentions found!\")\n",
        "            return []\n",
        "\n",
        "        print(f\"Found {len(contexts_data)} party mentions across articles\")\n",
        "\n",
        "        # Extract contexts for batch sentiment analysis\n",
        "        contexts = [item['context'] for item in contexts_data]\n",
        "\n",
        "        # Perform batch sentiment analysis\n",
        "        print(\"\\nPerforming batch sentiment analysis...\")\n",
        "        sentiment_results = self.batch_sentiment_analysis(contexts)\n",
        "\n",
        "        # Process results\n",
        "        print(\"\\nProcessing framing and opinion analysis...\")\n",
        "        results_list = []\n",
        "        article_results_dict = {}\n",
        "\n",
        "        for item, (sentiment, score) in tqdm(zip(contexts_data, sentiment_results), total=len(contexts_data)):\n",
        "            party = item['party']\n",
        "            context = item['context']\n",
        "            article_id = item['article_id']\n",
        "\n",
        "            # Opinion detection\n",
        "            opinion_data = self.detect_opinion_markers(context)\n",
        "\n",
        "            # Framing analysis\n",
        "            framing_data = self.analyze_framing(context)\n",
        "\n",
        "            # Store results\n",
        "            self.results[party]['sentiments'].append(sentiment)\n",
        "            self.results[party]['sentiment_scores'].append(score)\n",
        "            self.results[party]['opinions'].append(opinion_data['has_opinion'])\n",
        "            self.results[party]['opinion_counts'].append(opinion_data['opinion_count'])\n",
        "            self.results[party]['framing_scores'].append(framing_data['framing_score'])\n",
        "            self.results[party]['negative_frames'].append(framing_data['negative_frames'])\n",
        "            self.results[party]['positive_frames'].append(framing_data['positive_frames'])\n",
        "\n",
        "            # Track per-article results\n",
        "            if article_id not in article_results_dict:\n",
        "                article_results_dict[article_id] = {}\n",
        "\n",
        "            article_results_dict[article_id][party] = {\n",
        "                'sentiment': sentiment,\n",
        "                'sentiment_score': score,\n",
        "                'opinion_detected': opinion_data['has_opinion'],\n",
        "                'framing_score': framing_data['framing_score']\n",
        "            }\n",
        "\n",
        "        # Convert to list format\n",
        "        for article_id, analysis in article_results_dict.items():\n",
        "            results_list.append({\n",
        "                'article_id': article_id,\n",
        "                'analysis': analysis\n",
        "            })\n",
        "\n",
        "        self.articles_analyzed = len(article_results_dict)\n",
        "        print(f\"\\nAnalyzed {self.articles_analyzed} articles with party mentions\")\n",
        "\n",
        "        return results_list\n",
        "\n",
        "    def calculate_bias_metrics(self):\n",
        "        \"\"\"Calculate comprehensive bias metrics\"\"\"\n",
        "        metrics = {}\n",
        "\n",
        "        for party in ['bjp', 'congress']:\n",
        "            if not self.results[party]['sentiments']:\n",
        "                metrics[party] = None\n",
        "                continue\n",
        "\n",
        "            sentiments = self.results[party]['sentiments']\n",
        "            sentiment_counter = Counter(sentiments)\n",
        "            total = len(sentiments)\n",
        "\n",
        "            # Sentiment distribution\n",
        "            positive_pct = (sentiment_counter['positive'] / total) * 100\n",
        "            negative_pct = (sentiment_counter['negative'] / total) * 100\n",
        "            neutral_pct = (sentiment_counter['neutral'] / total) * 100\n",
        "\n",
        "            # Average sentiment score\n",
        "            avg_sentiment_score = np.mean(self.results[party]['sentiment_scores'])\n",
        "\n",
        "            # Opinion metrics\n",
        "            opinion_pct = (sum(self.results[party]['opinions']) / total) * 100\n",
        "            avg_opinion_count = np.mean(self.results[party]['opinion_counts'])\n",
        "\n",
        "            # Framing metrics\n",
        "            avg_framing_score = np.mean(self.results[party]['framing_scores'])\n",
        "            avg_negative_frames = np.mean(self.results[party]['negative_frames'])\n",
        "            avg_positive_frames = np.mean(self.results[party]['positive_frames'])\n",
        "\n",
        "            # Bias score (combined metric)\n",
        "            bias_score = (positive_pct - negative_pct) / 100\n",
        "            bias_score -= (opinion_pct / 100) * 0.3\n",
        "            bias_score += avg_framing_score * 0.2\n",
        "\n",
        "            metrics[party] = {\n",
        "                'total_mentions': total,\n",
        "                'sentiment_distribution': {\n",
        "                    'positive': positive_pct,\n",
        "                    'negative': negative_pct,\n",
        "                    'neutral': neutral_pct\n",
        "                },\n",
        "                'avg_sentiment_score': avg_sentiment_score,\n",
        "                'opinion_percentage': opinion_pct,\n",
        "                'avg_opinion_markers_per_article': avg_opinion_count,\n",
        "                'framing': {\n",
        "                    'avg_framing_score': avg_framing_score,\n",
        "                    'avg_negative_frames': avg_negative_frames,\n",
        "                    'avg_positive_frames': avg_positive_frames\n",
        "                },\n",
        "                'overall_bias_score': bias_score\n",
        "            }\n",
        "\n",
        "        # Comparative metrics\n",
        "        if metrics['bjp'] and metrics['congress']:\n",
        "            metrics['comparative'] = {\n",
        "                'coverage_ratio_bjp_to_congress': metrics['bjp']['total_mentions'] / metrics['congress']['total_mentions'],\n",
        "                'sentiment_gap': metrics['bjp']['sentiment_distribution']['positive'] - metrics['congress']['sentiment_distribution']['positive'],\n",
        "                'bias_score_difference': metrics['bjp']['overall_bias_score'] - metrics['congress']['overall_bias_score']\n",
        "            }\n",
        "\n",
        "        self.metrics = metrics\n",
        "        return metrics\n",
        "\n",
        "    def generate_report(self):\n",
        "        \"\"\"Generate human-readable report\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"POLITICAL BIAS ANALYSIS REPORT\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "        print(f\"Total Articles Analyzed: {self.articles_analyzed}\")\n",
        "        print(f\"Batch Size Used: {self.batch_size}\")\n",
        "        print(f\"Device Used: {self.device}\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        for party in ['bjp', 'congress']:\n",
        "            party_name = party.upper()\n",
        "            print(f\"\\n{party_name} ANALYSIS\")\n",
        "            print(\"-\"*70)\n",
        "\n",
        "            if not self.metrics.get(party):\n",
        "                print(f\"No mentions of {party_name} found in dataset\")\n",
        "                continue\n",
        "\n",
        "            m = self.metrics[party]\n",
        "\n",
        "            print(f\"Total Mentions: {m['total_mentions']}\")\n",
        "            print(f\"\\nSentiment Distribution:\")\n",
        "            print(f\"  Positive: {m['sentiment_distribution']['positive']:.2f}%\")\n",
        "            print(f\"  Negative: {m['sentiment_distribution']['negative']:.2f}%\")\n",
        "            print(f\"  Neutral:  {m['sentiment_distribution']['neutral']:.2f}%\")\n",
        "\n",
        "            print(f\"\\nOpinion Analysis:\")\n",
        "            print(f\"  Articles with Opinion Markers: {m['opinion_percentage']:.2f}%\")\n",
        "            print(f\"  Avg Opinion Markers per Article: {m['avg_opinion_markers_per_article']:.2f}\")\n",
        "\n",
        "            print(f\"\\nFraming Analysis:\")\n",
        "            print(f\"  Avg Framing Score: {m['framing']['avg_framing_score']:.2f}\")\n",
        "            print(f\"  Avg Negative Frames: {m['framing']['avg_negative_frames']:.2f}\")\n",
        "            print(f\"  Avg Positive Frames: {m['framing']['avg_positive_frames']:.2f}\")\n",
        "\n",
        "            print(f\"\\nOverall Bias Score: {m['overall_bias_score']:.3f}\")\n",
        "            print(f\"  (Range: -1 to +1, where +1 is most positive)\")\n",
        "\n",
        "        if 'comparative' in self.metrics:\n",
        "            print(f\"\\n{'='*70}\")\n",
        "            print(\"COMPARATIVE ANALYSIS\")\n",
        "            print(\"-\"*70)\n",
        "            comp = self.metrics['comparative']\n",
        "            print(f\"Coverage Ratio (BJP:Congress): {comp['coverage_ratio_bjp_to_congress']:.2f}:1\")\n",
        "            print(f\"Sentiment Gap (BJP - Congress): {comp['sentiment_gap']:.2f}%\")\n",
        "            print(f\"Bias Score Difference: {comp['bias_score_difference']:.3f}\")\n",
        "\n",
        "            # Interpretation\n",
        "            print(f\"\\nInterpretation:\")\n",
        "            if abs(comp['bias_score_difference']) < 0.1:\n",
        "                print(\"  → Relatively balanced coverage\")\n",
        "            elif comp['bias_score_difference'] > 0.1:\n",
        "                print(\"  → Slight bias towards BJP\")\n",
        "            else:\n",
        "                print(\"  → Slight bias towards Congress\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "    def save_model(self, output_path='bias_analysis_results'):\n",
        "        \"\"\"Save all results, metrics, and model state\"\"\"\n",
        "        print(f\"\\nSaving results to {output_path}...\")\n",
        "\n",
        "        import os\n",
        "        os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "        # Save metrics as JSON\n",
        "        with open(f'{output_path}/bias_metrics.json', 'w') as f:\n",
        "            json.dump(self.metrics, f, indent=4)\n",
        "\n",
        "        # Save raw results\n",
        "        with open(f'{output_path}/raw_results.pkl', 'wb') as f:\n",
        "            pickle.dump(self.results, f)\n",
        "\n",
        "        # Save configuration\n",
        "        config = {\n",
        "            'bjp_entities': list(self.BJP_ENTITIES),\n",
        "            'congress_entities': list(self.CONGRESS_ENTITIES),\n",
        "            'opinion_markers': list(self.OPINION_MARKERS),\n",
        "            'articles_analyzed': self.articles_analyzed,\n",
        "            'batch_size': self.batch_size,\n",
        "            'device': str(self.device),\n",
        "            'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "        }\n",
        "\n",
        "        with open(f'{output_path}/config.json', 'w') as f:\n",
        "            json.dump(config, f, indent=4)\n",
        "\n",
        "        # Save detailed CSV report\n",
        "        report_data = []\n",
        "        for party in ['bjp', 'congress']:\n",
        "            if self.metrics.get(party):\n",
        "                m = self.metrics[party]\n",
        "                report_data.append({\n",
        "                    'Party': party.upper(),\n",
        "                    'Total_Mentions': m['total_mentions'],\n",
        "                    'Positive_%': m['sentiment_distribution']['positive'],\n",
        "                    'Negative_%': m['sentiment_distribution']['negative'],\n",
        "                    'Neutral_%': m['sentiment_distribution']['neutral'],\n",
        "                    'Opinion_%': m['opinion_percentage'],\n",
        "                    'Framing_Score': m['framing']['avg_framing_score'],\n",
        "                    'Bias_Score': m['overall_bias_score']\n",
        "                })\n",
        "\n",
        "        pd.DataFrame(report_data).to_csv(f'{output_path}/summary_report.csv', index=False)\n",
        "\n",
        "        print(f\"✓ Saved bias_metrics.json\")\n",
        "        print(f\"✓ Saved raw_results.pkl\")\n",
        "        print(f\"✓ Saved config.json\")\n",
        "        print(f\"✓ Saved summary_report.csv\")\n",
        "        print(f\"\\nAll results saved to: {output_path}/\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "\n",
        "    # Initialize analyzer with batch processing\n",
        "    # Adjust batch_size based on your GPU memory (32 is good for most GPUs)\n",
        "    analyzer = PoliticalBiasAnalyzer(batch_size=32)\n",
        "\n",
        "    # Load your dataset\n",
        "    analyzer.load_data(\n",
        "        '/content/indian_express_political_article_one_year_scraped.csv',\n",
        "        text_column='News Content'\n",
        "    )\n",
        "\n",
        "    # Analyze all articles (now uses batch processing)\n",
        "    analyzer.analyze_dataset()\n",
        "\n",
        "    # Calculate bias metrics\n",
        "    analyzer.calculate_bias_metrics()\n",
        "\n",
        "    # Generate and print report\n",
        "    analyzer.generate_report()\n",
        "\n",
        "    # Save results\n",
        "    analyzer.save_model('bias_analysis_results')\n",
        "\n",
        "    print(\"\\n✓ Analysis complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHg52FH2lRZa",
        "outputId": "38150fc6-9997-412b-ce35-c861f836bed8"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing Political Bias Analyzer...\n",
            "Using device: cuda\n",
            "Loading sentiment analysis model...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data from /content/indian_express_political_article_one_year_scraped.csv...\n",
            "Loaded 4994 articles\n",
            "\n",
            "Analyzing 4994 articles...\n",
            "\n",
            "Preprocessing articles to extract party contexts...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4994/4994 [17:59<00:00,  4.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9525 party mentions across articles\n",
            "\n",
            "Performing batch sentiment analysis...\n",
            "\n",
            "Processing framing and opinion analysis...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9525/9525 [00:00<00:00, 20082.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analyzed 4971 articles with party mentions\n",
            "\n",
            "======================================================================\n",
            "POLITICAL BIAS ANALYSIS REPORT\n",
            "======================================================================\n",
            "Analysis Date: 2025-11-28 22:51:11\n",
            "Total Articles Analyzed: 4971\n",
            "Batch Size Used: 32\n",
            "Device Used: cuda\n",
            "======================================================================\n",
            "\n",
            "BJP ANALYSIS\n",
            "----------------------------------------------------------------------\n",
            "Total Mentions: 4701\n",
            "\n",
            "Sentiment Distribution:\n",
            "  Positive: 6.96%\n",
            "  Negative: 11.02%\n",
            "  Neutral:  82.03%\n",
            "\n",
            "Opinion Analysis:\n",
            "  Articles with Opinion Markers: 33.25%\n",
            "  Avg Opinion Markers per Article: 0.18\n",
            "\n",
            "Framing Analysis:\n",
            "  Avg Framing Score: 0.30\n",
            "  Avg Negative Frames: 0.24\n",
            "  Avg Positive Frames: 0.54\n",
            "\n",
            "Overall Bias Score: -0.080\n",
            "  (Range: -1 to +1, where +1 is most positive)\n",
            "\n",
            "CONGRESS ANALYSIS\n",
            "----------------------------------------------------------------------\n",
            "Total Mentions: 4824\n",
            "\n",
            "Sentiment Distribution:\n",
            "  Positive: 7.13%\n",
            "  Negative: 11.38%\n",
            "  Neutral:  81.49%\n",
            "\n",
            "Opinion Analysis:\n",
            "  Articles with Opinion Markers: 34.43%\n",
            "  Avg Opinion Markers per Article: 0.18\n",
            "\n",
            "Framing Analysis:\n",
            "  Avg Framing Score: 0.35\n",
            "  Avg Negative Frames: 0.21\n",
            "  Avg Positive Frames: 0.56\n",
            "\n",
            "Overall Bias Score: -0.076\n",
            "  (Range: -1 to +1, where +1 is most positive)\n",
            "\n",
            "======================================================================\n",
            "COMPARATIVE ANALYSIS\n",
            "----------------------------------------------------------------------\n",
            "Coverage Ratio (BJP:Congress): 0.97:1\n",
            "Sentiment Gap (BJP - Congress): -0.18%\n",
            "Bias Score Difference: -0.004\n",
            "\n",
            "Interpretation:\n",
            "  → Relatively balanced coverage\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Saving results to bias_analysis_results...\n",
            "✓ Saved bias_metrics.json\n",
            "✓ Saved raw_results.pkl\n",
            "✓ Saved config.json\n",
            "✓ Saved summary_report.csv\n",
            "\n",
            "All results saved to: bias_analysis_results/\n",
            "\n",
            "✓ Analysis complete!\n"
          ]
        }
      ]
    }
  ]
}